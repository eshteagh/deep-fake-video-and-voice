{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:13:25.977593Z",
     "start_time": "2019-10-15T19:13:25.962631Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------- general -------------\n",
    "debugging=True # executes the debugging (short) sections, prints results\n",
    "# debugging=False\n",
    "torch_manual_seed=0 # integer or None for no seed; for torch reproducibility, as much as possible\n",
    "#torch_manual_seed=None\n",
    "\n",
    "#--------- data -------------\n",
    "# images_folder_path=r'D:\\AI Data\\DeepFake\\ZioNLight Bibi.mp4 214x384 frames'\n",
    "images_folder_path=r'D:\\AI Data\\DeepFake\\ZioNLight Bibi.mp4 428x768 frames'\n",
    "\n",
    "\"\"\"restricting to soft data augmentation - color jitter (no random cropping or flipping - to keep all images aligned!)\n",
    "\"\"\"\n",
    "# random_transforms='train & val' # data augmentation on both train and val phases\n",
    "# random_transforms='train' # data augmentation only on train phase, validation is free of random transforms \n",
    "random_transforms='none' # no data augmentation\n",
    "\n",
    "#max_dataset_length=100 # if positive: builds a dataset by sampling only max_dataset_length samples from all available data; requires user approval\n",
    "max_dataset_length=0 # if non-positive: not restricting dataset length - using all available data\n",
    "seed_for_dataset_downsampling=0 # integer or None for no seed; for sampling max_dataset_length samples from dataset\n",
    "\n",
    "validation_ratio=0.3 # validation dataset ratio from total dataset length\n",
    "\n",
    "#batch_size_int_or_ratio_float=1e-2 # if float: batch_size=round(batch_size_over_dataset_length*len(dataset_to_split))\n",
    "batch_size_int_or_ratio_float=8 # if int: this is the batch size, should be 2**n\n",
    "data_workers=0 # 0 means no multiprocessing in dataloaders\n",
    "#data_workers='cpu cores' # sets data_workers=multiprocessing.cpu_count()\n",
    "\n",
    "shuffle_dataset_indices_for_split=True # dataset indices for dataloaders are shuffled before splitting to train and validation indices\n",
    "#shuffle_dataset_indices_for_split=False\n",
    "dataset_shuffle_random_seed=0 # numpy seed for sampling the indices for the dataset, before splitting to train and val dataloaders\n",
    "#dataset_shuffle_random_seed=None\n",
    "dataloader_shuffle=True # samples are shuffled inside each dataloader, on each epoch\n",
    "#dataloader_shuffle=False\n",
    "\n",
    "#--------- net -------------\n",
    "#net_architecture='simple auto-encoder'\n",
    "net_architecture='experimental auto-encoder'\n",
    "\n",
    "loss_name='MSE'\n",
    "\n",
    "#--------- training -------------\n",
    "train_model_else_load_weights=True\n",
    "#train_model_else_load_weights=False # instead of training, loads a pre-trained model and uses it\n",
    "\n",
    "force_train_evaluation_after_each_epoch=True # adding evaluation of the training dataset after each epoch finishes training\n",
    "# force_train_evaluation_after_each_epoch=False # default\n",
    "\n",
    "epochs=5\n",
    "learning_rate=1e-1\n",
    "\n",
    "optimizer_name='SGD'\n",
    "SGD_momentum=0.7 # default: 0.9\n",
    "\n",
    "# optimizer_name='Adam'\n",
    "Adam_betas=(0.7,0.98) # default: (0.9,0.999)\n",
    "\n",
    "lr_scheduler_decay_factor=0.9 # applies to all optimizers; on each lr_scheduler_step_size epochs, learning_rate*=lr_scheduler_decay_factor\n",
    "lr_scheduler_step_size=1\n",
    "\n",
    "best_model_criterion='min val epoch MSE' # criterion for choosing best net weights during training as the final weights\n",
    "return_to_best_weights_in_the_end=True # when training complets, loads weights of the best net, definied by best_model_criterion\n",
    "#return_to_best_weights_in_the_end=False\n",
    "\n",
    "period_in_seconds_to_log_loss=30 # <=0 means no logging during training, else: inter-epoch logging and reporting loss and metrics during training\n",
    "#plot_realtime_stats_on_logging=True # incomplete implementation!\n",
    "plot_realtime_stats_on_logging=False\n",
    "#plot_realtime_stats_after_each_epoch=True\n",
    "plot_realtime_stats_after_each_epoch=False\n",
    "#plot_loss_in_log_scale=True\n",
    "plot_loss_in_log_scale=False\n",
    "\n",
    "#offer_mode_saving=True # offer model weights saving ui after training (only if train_model_else_load_weights=True)\n",
    "offer_mode_saving=False\n",
    "models_folder_path='D:\\AI Data\\DeepFake\\Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:13:33.721614Z",
     "start_time": "2019-10-15T19:13:28.033223Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-15 22:13:33 <module> (INFO): script initialized\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(funcName)s (%(levelname)s): %(message)s',\n",
    "                   datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger=logging.getLogger('data processing logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "import copy\n",
    "import PIL\n",
    "import multiprocessing\n",
    "if data_workers=='cpu cores':\n",
    "    data_workers=multiprocessing.cpu_count()\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(torch_manual_seed)\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def plot_from_image_filenames_list(sample_indices_to_plot,image_filenames_list,\n",
    "                                       images_folder_path,images_per_row=4):\n",
    "    assert (isinstance(images_per_row,int) and images_per_row>0), 'images_per_row is invalid, must be a positive integer'\n",
    "    plt.figure()\n",
    "    columns_num=math.ceil(len(sample_indices_to_plot)/images_per_row)\n",
    "    for i,sample_index in enumerate(sample_indices_to_plot):\n",
    "        image_filename=image_filenames_list[sample_index]\n",
    "        image_array=plt.imread(os.path.join(images_folder_path,image_filename))    \n",
    "        \n",
    "        plt.subplot(columns_num,images_per_row,i+1)\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(image_filename)\n",
    "        plt.xticks(ticks=[])\n",
    "        plt.yticks(ticks=[])\n",
    "    plt.show()\n",
    "\n",
    "class images_torch_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,image_filenames,images_folder_path,transform_func=None):\n",
    "        self.image_filenames=image_filenames\n",
    "        self.images_folder_path=images_folder_path\n",
    "        self.transform_func=transform_func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image_filename=self.image_filenames[idx]\n",
    "        image_path=os.path.join(self.images_folder_path,image_filename)\n",
    "        image_array=PIL.Image.open(image_path)\n",
    "        \n",
    "        if self.transform_func!=None:\n",
    "            image_array=self.transform_func(image_array)\n",
    "        \n",
    "        sample={'image filename':image_filename,'image array':image_array}\n",
    "        return sample\n",
    "\n",
    "def plot_from_torch_dataset(sample_indices_to_plot,torch_dataset,\n",
    "                            images_per_row=4,image_format='PIL->torch'):\n",
    "    assert (isinstance(images_per_row,int) and images_per_row>0), 'images_per_row is invalid, must be a positive integer'\n",
    "    plt.figure()\n",
    "    columns_num=math.ceil(len(sample_indices_to_plot)/images_per_row)\n",
    "    for i,sample_index in enumerate(sample_indices_to_plot):\n",
    "        sample=torch_dataset[sample_index]\n",
    "        image_filename=sample['image filename']\n",
    "        image_array=sample['image array']\n",
    "        if image_format=='np->torch': # to return from a torch format that reached from a np format, to a np for plotting. see # Helper function to show a batch from https://pytorch.org/tutorials/beginner/data_loading_tutorial\n",
    "            image_array=image_array.transpose((1,2,0))\n",
    "        elif image_format=='PIL->torch': # to return from a torch format that reached from a PIL format, to a np for plotting. see # Helper function to show a batch from https://pytorch.org/tutorials/beginner/data_loading_tutorial\n",
    "            image_array=image_array.numpy().transpose((1,2,0))\n",
    "        \n",
    "        plt.subplot(columns_num,images_per_row,i+1)\n",
    "        plt.imshow(image_array)\n",
    "        plt.title(image_filename)\n",
    "        plt.xticks(ticks=[])\n",
    "        plt.yticks(ticks=[])\n",
    "    plt.show()\n",
    "\n",
    "def training_stats_plot(stats_dict,fig,loss_subplot,MSE_subplot,plot_loss_in_log_scale=False):\n",
    "    running_stats_df=pd.DataFrame.from_dict(stats_dict['train']['running metrics'],orient='index')\n",
    "    epoch_train_stats_df=pd.DataFrame.from_dict(stats_dict['train']['epoch metrics'],orient='index')\n",
    "    epoch_val_stats_df=pd.DataFrame.from_dict(stats_dict['val']['epoch metrics'],orient='index')\n",
    "    \n",
    "    loss_subplot.clear() # clearing plot before plotting, to avoid over-plotting\n",
    "    if len(running_stats_df)>0:\n",
    "        loss_subplot.plot(running_stats_df['loss per sample'],'-x',label='running train')\n",
    "    loss_subplot.plot(epoch_train_stats_df['loss per sample'],'k-o',label='epoch train')\n",
    "    loss_subplot.plot(epoch_val_stats_df['loss per sample'],'r-o',label='epoch val')\n",
    "    loss_subplot.set_ylabel('loss per sample')\n",
    "    loss_subplot.set_xlabel('epoch')\n",
    "    loss_subplot.grid()\n",
    "    loss_subplot.legend(loc='best')\n",
    "    if plot_loss_in_log_scale:\n",
    "        loss_subplot.set_yscale('log')\n",
    "        \n",
    "    MSE_subplot.clear() # clearing plot before plotting, to avoid over-plotting\n",
    "    if len(running_stats_df)>0:\n",
    "        MSE_subplot.plot(running_stats_df['MSE']**0.5,'-x',label='running train')\n",
    "    MSE_subplot.plot(epoch_train_stats_df['MSE']**0.5,'k-o',label='epoch train')\n",
    "    MSE_subplot.plot(epoch_val_stats_df['MSE']**0.5,'r-o',label='epoch val')\n",
    "    MSE_subplot.set_ylabel('sqrt(MSE)')\n",
    "    MSE_subplot.set_xlabel('epoch')\n",
    "    MSE_subplot.grid()\n",
    "    MSE_subplot.legend(loc='best')\n",
    "    if plot_loss_in_log_scale:\n",
    "        MSE_subplot.set_yscale('log')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "class remainder_time:\n",
    "    def __init__(self,time_seconds):\n",
    "        self.time_seconds=time_seconds\n",
    "        self.hours=int(time_seconds/3600)\n",
    "        self.remainder_minutes=int((time_seconds-self.hours*3600)/60)\n",
    "        self.remainder_seconds=time_seconds-self.hours*3600-self.remainder_minutes*60\n",
    "\n",
    "logger.info('script initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:13:33.757523Z",
     "start_time": "2019-10-15T19:13:33.724593Z"
    }
   },
   "outputs": [],
   "source": [
    "# a charm for interactive plotting in Jupyter notebook (useful for zooming, rotating 3D plots):\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:13:33.822358Z",
     "start_time": "2019-10-15T19:13:33.761495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\t\ttested on 1.3.0\t\tcurrent: 1.3.0\n",
      "Python\t\ttested on 3.7.3\t\tcurrent: 3.7.3\n",
      "numpy\t\ttested on 1.16.4\tcurrent: 1.16.4\n",
      "pandas\t\ttested on 0.24.2\tcurrent: 0.24.2\n",
      "matplotlib\ttested on 3.1.0\t\tcurrent: 3.1.0\n"
     ]
    }
   ],
   "source": [
    "print('torch\\t\\ttested on 1.3.0\\t\\tcurrent:',torch.__version__)\n",
    "\n",
    "import sys\n",
    "print('Python\\t\\ttested on 3.7.3\\t\\tcurrent:',sys.version[:sys.version.find(' ')])\n",
    "print('numpy\\t\\ttested on 1.16.4\\tcurrent:',np.__version__)\n",
    "print('pandas\\t\\ttested on 0.24.2\\tcurrent:',pd.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print('matplotlib\\ttested on 3.1.0\\t\\tcurrent:',matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:21:19.915653Z",
     "start_time": "2019-10-15T19:21:19.766119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-15 22:21:19 <module> (INFO): checking image shapes of 5 sampled images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 5977.jpg shape: (428, 768, 3)\n",
      "frame 2824.jpg shape: (428, 768, 3)\n",
      "frame 4264.jpg shape: (428, 768, 3)\n",
      "frame 4067.jpg shape: (428, 768, 3)\n",
      "frame 6157.jpg shape: (428, 768, 3)\n"
     ]
    }
   ],
   "source": [
    "image_num_to_sample=5\n",
    "# end of inputs ---------------------------------------------------------------\n",
    "\n",
    "image_filenames=os.listdir(images_folder_path)\n",
    "\n",
    "if max_dataset_length>0 and max_dataset_length<len(image_filenames):\n",
    "    user_data_approval=input('ATTENTION: downsampling is chosen - building a dataset by sampling only max_dataset_length=%d samples from all available data! approve? y/[n] '%(round(max_dataset_length)))\n",
    "    if user_data_approval!='y':\n",
    "        raise RuntimeError('user did not approve dataset max_dataset_length sampling!')\n",
    "    random.seed(seed_for_dataset_downsampling)\n",
    "    image_filenames=random.sample(image_filenames,max_dataset_length)\n",
    "\n",
    "if debugging:    \n",
    "    logger.info('checking image shapes of %d sampled images'%image_num_to_sample)\n",
    "    sampled_image_filenames=random.sample(image_filenames,image_num_to_sample)\n",
    "    for image_filename in sampled_image_filenames:\n",
    "        image_array=plt.imread(os.path.join(images_folder_path,image_filename))\n",
    "        print(f'{image_filename} shape:',image_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:31:16.693160Z",
     "start_time": "2019-10-15T19:31:16.684190Z"
    }
   },
   "source": [
    "### Data augmentation\n",
    "* Random transforms used below: color jitter (no random cropping or flipping - to keep all images aligned).\n",
    "* `random_transforms` set in [# Setting main parameteres](#Setting-main-parameters) controls data augmentation:\n",
    "    * `random_transforms='train & val'`: data augmentation on both train and val phases. The dataset that is created here is later split to training and validation datasets (and dataloaders).\n",
    "    * `random_transforms='train'`: data augmentation only on train phase, validation is free of random transforms. The dataset that is created here with transforms, and later new separate train/val datasets (and dataloaders) are created, with/without random transforms.\n",
    "    * `random_transforms='none'`: no data augmentation. The dataset that is created here is later split to training and validation datasets (and dataloaders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:28:00.616458Z",
     "start_time": "2019-10-15T19:27:57.994241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supply 0<crop_px<=min_img_dim=428 for torchvision.transforms.CenterCrop(crop_px): 400\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transform_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-67cbafc13744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \"\"\"\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mimages_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages_torch_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_filenames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages_folder_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtransform_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image array'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0msample_pixels_per_channel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msample_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform_func' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"torchvision.transforms accept PIL images, and not np images that are \n",
    "    created when using skimage as presented in \n",
    "    https://pytorch.org/tutorials/beginner/data_loading_tutorial\n",
    "\n",
    "torchvision transforms: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\"\"\"\n",
    "min_img_dim=min(image_array.shape[0],image_array.shape[1])\n",
    "crop_px=int(input('Supply 0<crop_px<=min_img_dim=%d for torchvision.transforms.CenterCrop(crop_px): '%(min_img_dim)))\n",
    "assert crop_px>0,'crop_px<=0 supplied, correct to be >0!'\n",
    "assert crop_px<=min_img_dim,'supplied crop_px bigger than smallest image dimention (%d) ,correct!'%(min_img_dim)\n",
    "\n",
    "transform_func_with_random=torchvision.transforms.Compose([\n",
    "#            torchvision.transforms.Resize(400),\n",
    "#            torchvision.transforms.RandomCrop(390),\n",
    "            torchvision.transforms.CenterCrop(crop_px),\n",
    "            torchvision.transforms.ColorJitter(brightness=0.1,contrast=0.1,saturation=0,hue=0),\n",
    "#            torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            ])\n",
    "transform_func_no_random=torchvision.transforms.Compose([\n",
    "            torchvision.transforms.CenterCrop(crop_px),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            ])\n",
    "\"\"\"torchvision.transforms.ToTensor() Converts a PIL Image or numpy.ndarray (H x W x C) in the \n",
    "    range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the \n",
    "    range [0.0, 1.0] if the PIL Image belongs to one of the modes \n",
    "    (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has \n",
    "    dtype = np.uint8\n",
    "In the other cases, tensors are returned without scaling\n",
    "source: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\"\"\"\n",
    "# if random_transforms=='none':\n",
    "#     random_transforms_ui=input('random_transforms=False was set, meaning no data augmentation, approve? [y]/n ')\n",
    "#     if random_transforms_ui=='n':\n",
    "#         raise RuntimeError('user did not approve no data augmentation, aborting')\n",
    "\n",
    "if random_transforms=='none':\n",
    "    transform_func=transform_func_no_random\n",
    "else:\n",
    "    transform_func=transform_func_with_random\n",
    "    \n",
    "images_dataset=images_torch_dataset(image_filenames,images_folder_path,transform_func=transform_func)\n",
    "sample_size=images_dataset[0]['image array'].size()\n",
    "sample_pixels_per_channel=sample_size[1]*sample_size[2]\n",
    "sample_pixels_all_channels=sample_size[0]*sample_pixels_per_channel\n",
    "logger.info('set a PyTorch dataset of length %.2e, input size: (%d,%d,%d)'%(\n",
    "        len(image_filenames),sample_size[0],sample_size[1],sample_size[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging: verifying dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:40:50.136726Z",
     "start_time": "2019-10-14T15:40:49.762472Z"
    }
   },
   "outputs": [],
   "source": [
    "samples_to_plot=9\n",
    "#sampling_for_sample_verification='none' # plotting first samples_to_plot samples\n",
    "sampling_for_sample_verification='random' # plotting randomly selected samples_to_plot samples, using seed_for_sample_verification seed\n",
    "seed_for_sample_verification=0\n",
    "images_per_row=3\n",
    "# end of inputs ---------------------------------------------------------------\n",
    "if sampling_for_sample_verification=='none':\n",
    "    sample_indices_to_plot=range(samples_to_plot)\n",
    "elif sampling_for_sample_verification=='random':\n",
    "    random.seed(seed_for_sample_verification)\n",
    "    sample_indices_to_plot=random.sample(range(len(image_filenames)),samples_to_plot)\n",
    "else:\n",
    "    raise RuntimeError('unsupported sampling_for_sample_verification input!')\n",
    "\n",
    "if debugging:\n",
    "    plot_from_image_filenames_list(sample_indices_to_plot,image_filenames,\n",
    "                                   images_folder_path,images_per_row)\n",
    "    plt.suptitle('plotting images directly from disk')\n",
    "    \n",
    "    plot_from_torch_dataset(sample_indices_to_plot,images_dataset,\n",
    "                            images_per_row,image_format='PIL->torch')\n",
    "    plt.suptitle('plotting images from PyTorch dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting to train-val datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:40:50.161658Z",
     "start_time": "2019-10-14T15:40:50.138721Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_to_split=images_dataset\n",
    "\n",
    "if isinstance(batch_size_int_or_ratio_float,int):\n",
    "    batch_size=batch_size_int_or_ratio_float\n",
    "elif isinstance(batch_size_int_or_ratio_float,float):\n",
    "    batch_size=round(batch_size_int_or_ratio_float*len(dataset_to_split))\n",
    "else:\n",
    "    raise RuntimeError('unsupported batch_size input!')\n",
    "if batch_size<1:\n",
    "    batch_size=1\n",
    "    logger.warning('batch_size=round(batch_size_over_dataset_length*len(dataset_to_split))<1 so batch_size=1 was set')\n",
    "if batch_size==1:\n",
    "    user_batch_size=input('batch_size=1 should cause errors since batch_size>1 is generally assumed! enter a new batch size equal or larger than 1, or smaller than 1 to abort: ')\n",
    "    if user_batch_size<1:\n",
    "        raise RuntimeError('aborted by user batch size decision')\n",
    "    else:\n",
    "        batch_size=round(user_batch_size)\n",
    "\n",
    "dataset_length=len(dataset_to_split)\n",
    "dataset_indices=list(range(dataset_length))\n",
    "split_index=int((1-validation_ratio)*dataset_length)\n",
    "if shuffle_dataset_indices_for_split:\n",
    "    np.random.seed(dataset_shuffle_random_seed)\n",
    "    np.random.shuffle(dataset_indices)\n",
    "train_indices=dataset_indices[:split_index]\n",
    "val_indices=dataset_indices[split_index:]\n",
    "\n",
    "# splitting the dataset to train and val\n",
    "train_dataset=torch.utils.data.Subset(dataset_to_split,train_indices)\n",
    "val_dataset=torch.utils.data.Subset(dataset_to_split,val_indices)\n",
    "\n",
    "# creating the train and val dataloaders\n",
    "train_dataloader=torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,\n",
    "                        num_workers=data_workers,shuffle=dataloader_shuffle)\n",
    "val_dataloader=torch.utils.data.DataLoader(val_dataset,batch_size=batch_size,\n",
    "                        num_workers=data_workers,shuffle=dataloader_shuffle)\n",
    "\n",
    "# structuring\n",
    "dataset_indices={'train':train_indices,'val':val_indices}\n",
    "datasets={'train':train_dataset,'val':val_dataset}\n",
    "dataset_samples_number={'train':len(train_dataset),'val':len(val_dataset)}\n",
    "\n",
    "dataloaders={'train':train_dataloader,'val':val_dataloader}\n",
    "dataloader_batches_number={'train':len(train_dataloader),'val':len(val_dataloader)}\n",
    "\n",
    "logger.info('dataset split to training and validation datasets and dataloaders with validation_ratio=%.1f, lengths: (train,val)=(%d,%d)'%(\n",
    "        validation_ratio,dataset_samples_number['train'],dataset_samples_number['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging: verifying dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T15:40:50.709180Z",
     "start_time": "2019-10-14T15:40:50.164652Z"
    }
   },
   "outputs": [],
   "source": [
    "images_per_row=4\n",
    "# end of inputs ---------------------------------------------------------------\n",
    "\n",
    "if debugging:\n",
    "    if __name__=='__main__' or data_workers==0: # required in Windows for multi-processing\n",
    "        samples_batches={}\n",
    "        for phase in ['train','val']:\n",
    "            samples_batch=next(iter(dataloaders[phase]))\n",
    "            samples_batches.update({phase:samples_batch})\n",
    "    else:\n",
    "        raise RuntimeError('cannot use multiprocessing (data_workers>0 in dataloaders) in Windows when executed not as main!')\n",
    "        \n",
    "    columns_num=math.ceil(batch_size/images_per_row)\n",
    "    for phase in ['train','val']:\n",
    "        plt.figure()\n",
    "        for i in range(batch_size):\n",
    "            samples_batch=samples_batches[phase]\n",
    "            image_array=samples_batch['image array'][i].numpy().transpose((1,2,0))\n",
    "            image_filename=samples_batch['image filename'][i]\n",
    "            \n",
    "            plt.subplot(columns_num,images_per_row,i+1)\n",
    "            plt.imshow(image_array)\n",
    "            plt.title(image_filename)\n",
    "            plt.xticks(ticks=[])\n",
    "            plt.yticks(ticks=[])\n",
    "        plt.suptitle('plotting a batch from the %s dataloader'%phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:22:42.449827Z",
     "start_time": "2019-10-14T18:22:38.360489Z"
    }
   },
   "outputs": [],
   "source": [
    "if training_progress_ratio_to_log_loss>1:\n",
    "    raise RuntimeError('invalid training_progress_ratio_to_log_loss=%.2f, must be <=1'%training_progress_ratio_to_log_loss)\n",
    "period_in_batches_to_log_loss=round(training_progress_ratio_to_log_loss*dataset_samples_number['train']/batch_size) # logging only during training\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if plot_realtime_stats_on_logging or plot_realtime_stats_after_each_epoch:\n",
    "    logger.warning('plotting from inside the net loop is not working, should be debugged...')\n",
    "\n",
    "if net_architecture=='simple auto-encoder':\n",
    "    # inspired by https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "    class autoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(autoencoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3,16,20,stride=4,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(16),\n",
    "                \n",
    "                nn.Conv2d(16,3,8,stride=2,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(3),\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(3,16,8,stride=2,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(16),\n",
    "                \n",
    "                nn.ConvTranspose2d(16,3,20,stride=4,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(3),\n",
    "            )\n",
    "    \n",
    "        def forward(self,x):\n",
    "            x=self.encoder(x)\n",
    "            x=self.decoder(x)\n",
    "            return x\n",
    "    model=autoencoder()\n",
    "elif net_architecture=='experimental auto-encoder':\n",
    "    class autoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(autoencoder, self).__init__()\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3,16,4,stride=2,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(16),\n",
    "                \n",
    "                nn.Conv2d(16,8,4,stride=1,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(8),\n",
    "                \n",
    "                nn.Conv2d(8,4,4,stride=2,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(4),\n",
    "            )\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(4,8,4,stride=2,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(8),\n",
    "                \n",
    "                nn.ConvTranspose2d(8,16,4,stride=1,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(16),\n",
    "                \n",
    "                nn.ConvTranspose2d(16,3,4,stride=2,bias=False),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(3),\n",
    "            )\n",
    "    \n",
    "        def forward(self,x):\n",
    "            x=self.encoder(x)\n",
    "            x=self.decoder(x)\n",
    "            return x\n",
    "    model=autoencoder()\n",
    "else:\n",
    "    raise RuntimeError('untreated net_architecture!')\n",
    "\n",
    "model=model.to(device)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=momentum)\n",
    "\n",
    "if loss_name=='MSE':\n",
    "    loss_fn=nn.MSELoss(reduction='mean').to(device)\n",
    "else:\n",
    "    raise RuntimeError('untreated loss_name input')\n",
    "scheduler=torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "    step_size=lr_scheduler_step_size,gamma=lr_scheduler_decay_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging: verifying net outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:22:47.834108Z",
     "start_time": "2019-10-14T18:22:44.948344Z"
    }
   },
   "outputs": [],
   "source": [
    "if debugging:\n",
    "    if __name__=='__main__' or data_workers==0:\n",
    "        batch=next(iter(dataloaders['train']))\n",
    "        input_images=batch['image array']\n",
    "        input_images=input_images.to(device)\n",
    "        print('input shape:',input_images.shape)\n",
    "        model.eval()\n",
    "        output_images=model(input_images)\n",
    "        print('output shape:',output_images.shape)\n",
    "        print('nn.MSELoss(input_images,output_images):',\n",
    "              nn.MSELoss(reduction='mean').to(device)(input_images,output_images))\n",
    "        print('((input_images-output_images)**2).mean():',\n",
    "              ((input_images-output_images)**2).mean())\n",
    "    else:\n",
    "        raise RuntimeError('cannot use multiprocessing (data_workers>0 in dataloaders) in Windows when executed not as main!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:28:31.935636Z",
     "start_time": "2019-10-14T18:23:29.773988Z"
    }
   },
   "outputs": [],
   "source": [
    "if train_model_else_load_weights and (__name__=='__main__' or data_workers==0):\n",
    "    stats_dict={'train':{'epoch metrics':{},\n",
    "                     'running metrics':{}}, # running = measuerd on samples only since the last log\n",
    "                     'val':{'epoch metrics':{}}}\n",
    "    \n",
    "    total_batches=epochs*(dataloader_batches_number['train']+dataloader_batches_number['val'])\n",
    "    \n",
    "    pytorch_total_wts=sum(p.numel() for p in model.parameters())\n",
    "    pytorch_trainable_wts=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    logger.info(\"started training '%s' net on %s, trainable/total weigths: %d/%d\"%(\n",
    "        net_architecture,device,pytorch_trainable_wts,pytorch_total_wts))\n",
    "    tic=time()\n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train() # set model to training mode\n",
    "            else:\n",
    "                model.eval() # set model to evaluate mode\n",
    "            \n",
    "            epoch_loss=0.0 # must be a float\n",
    "            epoch_squared_error=0.0\n",
    "            samples_processed_since_last_log=0\n",
    "            loss_since_last_log=0.0 # must be a float\n",
    "            squared_error_since_last_log=0.0\n",
    "            \n",
    "            for i_batch,batch in enumerate(dataloaders[phase]):\n",
    "                input_images=batch['image array'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad() # zero the parameter gradients\n",
    "                \n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase=='train'): # if phase=='train' it tracks tensor history for grad calc\n",
    "                    output_images=model(input_images)\n",
    "                    loss=loss_fn(output_images,input_images)\n",
    "                    if torch.isnan(loss):\n",
    "                        raise RuntimeError('reached NaN loss - aborting training!')\n",
    "                    # backward + optimize if training\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # accumulating stats\n",
    "                samples_number=len(input_images)\n",
    "                samples_processed_since_last_log+=samples_number\n",
    "                \n",
    "                current_loss=loss.item()*samples_number*sample_pixels_all_channels # the loss is averaged across samples and pixels in each minibatch, so it is multiplied to return to a total\n",
    "                epoch_loss+=current_loss\n",
    "                loss_since_last_log+=current_loss\n",
    "                \n",
    "                with torch.set_grad_enabled(False):\n",
    "                    batch_squared_error=((output_images-input_images)**2).sum()\n",
    "                    batch_squared_error=batch_squared_error.item()\n",
    "                epoch_squared_error+=batch_squared_error\n",
    "                squared_error_since_last_log+=batch_squared_error\n",
    "                \n",
    "                if phase=='train' and i_batch%period_in_batches_to_log_loss==(period_in_batches_to_log_loss-1):\n",
    "                    loss_since_last_log_per_sample=loss_since_last_log/samples_processed_since_last_log\n",
    "                    MSE_since_last_log=squared_error_since_last_log/samples_processed_since_last_log\n",
    "            \n",
    "                    completed_batches=epoch*(dataloader_batches_number['train']+dataloader_batches_number['val'])+(i_batch+1)\n",
    "                    completed_batches_progress=completed_batches/total_batches\n",
    "                    passed_seconds=time()-tic\n",
    "                    expected_seconds=passed_seconds/completed_batches_progress*(1-completed_batches_progress)\n",
    "                    expected_remainder_time=remainder_time(expected_seconds)\n",
    "                    \n",
    "                    logger.info('(epoch %d/%d, batch %d/%d, %s) running loss per sample (since last log): %.3e, running sqrt(MSE) (since last log): sqrt(%.3e)=%.3e\\n\\tETA: %dh:%dm:%.0fs'%(\n",
    "                                epoch+1,epochs,i_batch+1,dataloader_batches_number[phase],phase,\n",
    "                                loss_since_last_log_per_sample,\n",
    "                                MSE_since_last_log,MSE_since_last_log**0.5,\n",
    "                                expected_remainder_time.hours,expected_remainder_time.remainder_minutes,expected_remainder_time.remainder_seconds))\n",
    "                    \n",
    "                    partial_epoch=epoch+completed_batches_progress\n",
    "                    stats_dict[phase]['running metrics'].update({partial_epoch:\n",
    "                        {'batch':i_batch+1,'loss per sample':loss_since_last_log_per_sample,\n",
    "                         'MSE':MSE_since_last_log}})\n",
    "                    \n",
    "                    loss_since_last_log=0.0 # must be a float\n",
    "                    squared_error_since_last_log=0.0\n",
    "                    samples_processed_since_last_log=0\n",
    "            \n",
    "            # epoch stats\n",
    "            epoch_loss_per_sample=epoch_loss/dataset_samples_number[phase]\n",
    "            epoch_MSE=epoch_squared_error/dataset_samples_number[phase]\n",
    "            \n",
    "            stats_dict[phase]['epoch metrics'].update({epoch:\n",
    "                        {'loss per sample':epoch_loss_per_sample,\n",
    "                         'MSE':epoch_MSE}})\n",
    "            if phase=='val':\n",
    "                if best_model_criterion=='min val epoch MSE':\n",
    "                    best_criterion_current_value=epoch_MSE\n",
    "                    if epoch==0:\n",
    "                        best_criterion_best_value=best_criterion_current_value\n",
    "                        best_model_wts=copy.deepcopy(model.state_dict())\n",
    "                        best_epoch=epoch\n",
    "                    else:\n",
    "                        if best_criterion_current_value<best_criterion_best_value:\n",
    "                            best_criterion_best_value=best_criterion_current_value\n",
    "                            best_model_wts=copy.deepcopy(model.state_dict())\n",
    "                            best_epoch=epoch\n",
    "                \n",
    "                completed_epochs_progress=(epoch+1)/epochs\n",
    "                passed_seconds=time()-tic\n",
    "                expected_seconds=passed_seconds/completed_epochs_progress*(1-completed_epochs_progress)\n",
    "                expected_remainder_time=remainder_time(expected_seconds)\n",
    "                \n",
    "                # not printing epoch stats for training, since in this phase they are being measured while the weights are being updated, unlike in validation where stats are measured with no update\n",
    "                logger.info('(epoch %d, %s) epoch loss per sample: %.3e, epoch sqrt(MSE): sqrt(%.3e)=%.3e\\n\\tETA: %dh:%dm:%.0fs'%(\n",
    "                                    epoch+1,phase,\n",
    "                                    epoch_loss_per_sample,\n",
    "                                    epoch_MSE,epoch_MSE**0.5,\n",
    "                                    expected_remainder_time.hours,\n",
    "                                    expected_remainder_time.remainder_minutes,\n",
    "                                    expected_remainder_time.remainder_seconds))\n",
    "                print('-'*10)\n",
    "    toc=time()\n",
    "    elapsed_sec=toc-tic\n",
    "\n",
    "    logger.info('finished training %d epochs in %dm:%.1fs'%(\n",
    "            epochs,elapsed_sec//60,elapsed_sec%60))\n",
    "    if return_to_best_weights_in_the_end:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        logger.info(\"loaded weights of best model according to '%s' criterion: best value %.3f achieved in epoch %d\"%(\n",
    "                best_model_criterion,best_criterion_best_value,best_epoch+1))\n",
    "    if not (plot_realtime_stats_on_logging or plot_realtime_stats_after_each_epoch):\n",
    "        fig=plt.figure()\n",
    "        plt.suptitle('model stats')\n",
    "        loss_subplot=plt.subplot(1,2,1)\n",
    "        MSE_subplot=plt.subplot(1,2,2)\n",
    "    training_stats_plot(stats_dict,fig,loss_subplot,MSE_subplot)\n",
    "else: # train_model_else_load_weights==False\n",
    "    ui_model_name=input('model weights file name to load: ')\n",
    "    model_weights_file_path=os.path.join(models_folder_path,ui_model_name)\n",
    "    if not os.path.isfile(model_weights_file_path):\n",
    "        raise RuntimeError('%model_weights_path does not exist!')\n",
    "    model_weights=torch.load(model_weights_file_path)\n",
    "    model.load_state_dict(model_weights)\n",
    "    logger.info('model weights from %s were loaded'%model_weights_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:30:18.273573Z",
     "start_time": "2019-10-14T18:29:40.060064Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"the validation class_metrics_df measured here in the model evaluation must be identical to those measured during the\n",
    "    last/best epoch, UNLIKE the training metrics - since the train phase metrics measured during training were being \n",
    "    measured while the weights were being updated in batches (!), not after the train phase epoch completed (which \n",
    "    would require another iteration on the train dataloader to measure metrics, as is done here without training)\n",
    "\"\"\"\n",
    "logger.info('started model evaluation')\n",
    "model.eval() # set model to evaluate mode\n",
    "for phase in ['train','val']:\n",
    "    epoch_loss=0.0 # must be a float\n",
    "    epoch_squared_error=0.0\n",
    "    \n",
    "    for i_batch,batch in enumerate(dataloaders[phase]):\n",
    "        input_images=batch['image array'].to(device)\n",
    "                    \n",
    "        # forward\n",
    "        with torch.set_grad_enabled(False): # if phase=='train' it tracks tensor history for grad calc\n",
    "            output_images=model(input_images)\n",
    "            loss=loss_fn(output_images,input_images)\n",
    "        \n",
    "        # accumulating stats\n",
    "        samples_number=len(input_images)            \n",
    "        current_loss=loss.item()*sample_pixels_all_channels # the loss is averaged across samples and pixels in each minibatch, so it is multiplied to return to a total\n",
    "        epoch_loss+=current_loss\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            batch_squared_error=((output_images-input_images)**2).sum()\n",
    "            batch_squared_error=batch_squared_error.item()\n",
    "        epoch_squared_error+=batch_squared_error\n",
    "    \n",
    "    # epoch stats\n",
    "    epoch_loss_per_sample=epoch_loss/dataset_samples_number[phase]\n",
    "    epoch_MSE=epoch_squared_error/dataset_samples_number[phase]\n",
    "    \n",
    "    logger.info('(post-training, %s) loss per sample: %.3e, sqrt(MSE): sqrt(%.3e)=%.3e'%(\n",
    "                    phase,epoch_loss_per_sample,epoch_MSE,epoch_MSE**0.5))\n",
    "          \n",
    "logger.info('completed model evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T18:30:58.224099Z",
     "start_time": "2019-10-14T18:30:58.021638Z"
    }
   },
   "outputs": [],
   "source": [
    "samples_to_plot=4\n",
    "#sampling_for_sample_verification='none' # plotting first samples_to_plot samples from datasets (train, val)\n",
    "sampling_for_sample_verification='random' # plotting randomly selected samples_to_plot samples from datasets (train, val), using seed_for_sample_verification seed\n",
    "seed_for_sample_verification=0\n",
    "images_per_row=2\n",
    "# end of inputs ---------------------------------------------------------------\n",
    "\n",
    "# concatenating the samples to inspect into batches\n",
    "image_batches={}\n",
    "for phase in ['train','val']:\n",
    "    if sampling_for_sample_verification=='none':\n",
    "        sample_indices_to_plot=range(samples_to_plot)\n",
    "    elif sampling_for_sample_verification=='random':\n",
    "        random.seed(seed_for_sample_verification)\n",
    "        sample_indices_to_plot=random.sample(range(len(datasets[phase])),samples_to_plot)\n",
    "    else:\n",
    "        raise RuntimeError('unsupported sampling_for_sample_verification input!')\n",
    "    \n",
    "    image_tensors_list=[]\n",
    "    for i,i_sample in enumerate(sample_indices_to_plot):\n",
    "        image_array=datasets[phase][i_sample]['image array'].unsqueeze(0)\n",
    "        image_tensors_list.append(image_array)\n",
    "    image_batches.update({phase:torch.cat(image_tensors_list,0)})\n",
    "\n",
    "# applying the model, plotting results\n",
    "model.eval() # set model to evaluate mode\n",
    "for phase in ['train','val']:\n",
    "    input_images=image_batches[phase].to(device)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        output_images=model(input_images)\n",
    "    \n",
    "    # see https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "    input_images_grid=np.transpose(vutils.make_grid(\n",
    "            input_images,nrow=images_per_row,padding=5,scale_each=True,normalize=True).cpu(),(1,2,0))\n",
    "    output_images_grid=np.transpose(vutils.make_grid(\n",
    "            output_images,nrow=images_per_row,padding=5,scale_each=True,normalize=True).cpu(),(1,2,0))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.title('original images')\n",
    "    plt.imshow(input_images_grid)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.title('reconstructed images')\n",
    "    plt.imshow(output_images_grid)\n",
    "\n",
    "    plt.suptitle('%s batch'%phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offer_mode_saving and train_model_else_load_weights:\n",
    "    try: os.mkdir(models_folder_path)\n",
    "    except FileExistsError: pass # if the folder exists already - do nothing\n",
    "    \n",
    "    saving_decision=input('save model weights? [y]/n ')\n",
    "    if saving_decision!='n':\n",
    "        ui_model_name=input('name model weights file: ')\n",
    "        model_weights_file_path=os.path.join(models_folder_path,ui_model_name+'.ptweights')\n",
    "        if os.path.isfile(model_weights_file_path):\n",
    "            alternative_filename=input('%s already exists, give a different file name to save, the same file name to over-write, or hit enter to abort: '%model_weights_file_path)\n",
    "            if alternative_filename=='':\n",
    "                raise RuntimeError('aborted by user')\n",
    "            else:\n",
    "                model_weights_file_path=os.path.join(models_folder_path,alternative_filename+'.ptweights')\n",
    "        torch.save(model.state_dict(),model_weights_file_path)       \n",
    "        logger.info('%s saved'%model_weights_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
